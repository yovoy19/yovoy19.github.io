{"meta":{"title":"Hexo","subtitle":"","description":"This is yovoy_blog","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"spark基础环境配置","slug":"Spark基础环境","date":"2022-05-21T09:45:28.537Z","updated":"2022-05-24T10:02:07.665Z","comments":true,"path":"2022/05/21/Spark基础环境/","link":"","permalink":"http://example.com/2022/05/21/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83/","excerpt":"","text":"spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载 一、基础环境1.导入三台虚拟机，配置如下 主机名 node1 node2 node3 IP 192.168.231.151 192.168.231.152 192.168.231.153 用户名、密码 root&#x2F;123456 root&#x2F;123456 root&#x2F;123456 2.集群角色规划 服务器 运行角色 node1.itcast.cn Namenode(主角色)，Datanode(从角色)，Resourcemanager(主角色)，Nodemanager(从角色)，master，follower，worker，QuorumPeerMain,sparksubmit node2.itcast.cn Secondarynamenode(主角色辅助角色)，datanode(从角色)，Nodemanager(从角色)，worker，leader node2.itcast.cn Datanode(从角色)，nodemanager(从角色)，worker，follower （一）主机hosts映射（三台）：1234567(base) [root@node1 ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.231.151 node1 node1.itcast.cn192.168.231.152 node2 node1.itcast.cn192.168.231.153 node3 node1.itcast.cn More info: Writing（1）关闭防火墙 1systemctl status firewalld.service 如下：[root@node1 ~]# systemctl status firewalld.service● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:firewalld(1)More info: Server（2）同步时间 1ntpdate ntp5.aliyun.com [root@node1 ~]# ntpdate ntp5.aliyun.com24 May 15:40:06 ntpdate[6487]: step time server 203.107.6.88 offset 0.770984 secYou have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root (二)配置ssh免密登录（三台）node1免密登录2、3 1ssh-keygen -t rsa(4次回车) [root@node1 ~]# ssh-keygen -t rsaGenerating public&#x2F;private rsa key pair.Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):&#x2F;root&#x2F;.ssh&#x2F;id_rsa already exists.Overwrite (y&#x2F;n)? yEnter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub.The key fingerprint is:SHA256:Um11dr+jE0C5kCbEYyoKqZpQBqMuqSF4Pq6WpnRGtg4 &#x72;&#111;&#111;&#116;&#64;&#110;&#x6f;&#100;&#101;&#x31;&#x2e;&#105;&#x74;&#x63;&#97;&#x73;&#x74;&#x2e;&#x63;&#x6e;The key’s randomart image is:+—[RSA 2048]—-+| o. ..o o .||o &#x3D; &#x3D;.o o ..||.+ o &#x3D; +.. .||+ o. . . . .. .||&#x3D;+.o. . S . o ||Oo+ . . o .||*E.+ o ||&#x3D;+B . ||*o.o |+—-[SHA256]—–+You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root 12ssh-copy-id node2 （输入node2密码）ssh-copy-id node3 （node3密码） (base) [root@node1 ~]# ssh-copy-id node2&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: Source of key(s) to be installed: “&#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub”&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: 1 key(s) remain to be installed – if you are prompted now it is to install the new keysroot@node2’s password: (输入node2密码) Number of key(s) added: 1 Now try logging into the machine, with: “ssh ‘node2’”and check to make sure that only the key(s) you wanted were added. You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root验证： 12ssh node2ssh node3 (base) [root@node1 ~]# ssh node2Last login: Fri May 13 08:33:21 2022 from 192.168.231.1 同理，node2，node3也一样。 二、安装配置JDK(1)编译环境软件安装目录 1mkdir -p /export/servers (2)上传、解压、安装jdk解压至&#x2F;export&#x2F;severs、 1tar -zxvf /export/servers/jdk-8u241-linux-x64.tar.gr -C /export/servers/ JDK安装目录重命名为jdk 1mv /export/servers/jdk1.8.0_241/ jdk More info: Generating(3)配置环境变量添加jdk配置： 1234#JDKexport JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar (4)重新加载环境变量并验证： 12source /etc/profilejava -version [root@node1 ~]# java -versionjava version “1.8.0_241”Java(TM) SE Runtime Environment (build 1.8.0_241-b07)Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)(5)分发JDK相关文件到node2，node31.jdk相关文件： 12scp -r /export/servers/jdk1.8.0_171/ root@node2:/export/servers/ scp -r /export/servers/jdk1.8.0_171/ root@node3:/export/servers/ 2.系统环境变量： 12scp -r /etc/profile root@node2:/etc/profile scp -r /etc/profile root@node3:/etc/profile 3.分别在node2、3重新使环境变量生效 1source /etc/profile 三、Hadoop集群的安装和配置(1)上传 Hadoop 安装包并解压 1tar zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz (2)修改配置文件 1cd /export/server/hadoop-3.3.0/etc/hadoop 1.hadoop-env.sh 12345678export JAVA_HOME=/export/server/jdk1.8.0_241#Hadoop各个组件启动运行身份export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root 2.core-site.xml 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt;&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 单位：分 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.hdfs-site.xml 1234567&lt;configuration&gt;&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 4.mapred-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;configuration&gt;&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 5.yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt;&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; (3)将hadoop添加到环境变量(三台) 123#HADOOP_HOMEexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 重新加载环境变量： 1source /etc/profile (4)向node2、3 分发hadoop目录 12scp -r /export/server/hadoop-3.3.0/ root@node2:/export/server/hadoop-3.3.0/scp -r /export/server/hadoop-3.3.0/ root@node3:/export/server/hadoop-3.3.0/ (5)在node1格式化hdfs 1hdfs namenode -format (6)启动hadoop集群 12start-dfs.shstart-yarn.sh （7）jps查看进程：[root@node2 ~]# jps5697 Jps4867 DataNode2183 QuorumPeerMain5065 NodeManager4973 SecondaryNameNodeWeb UI页面验证：yarn: http:&#x2F;&#x2F;主节点ip:9868&#x2F;hdfs: http:&#x2F;&#x2F;主节点ip:8088&#x2F;More info: Deployment 四、Zookeeper集群安装（1）上传、解压、安装zookeeper 1tar -zxvf /export/servers/zookeeper-3.4.6.tar.gr -C /export/servers/ 建立软连接 1ln -s zookeeper/zookeeper-3.4.6 （2）修改配置文件1.zoo.cfg 1234cd /export/server/zookeeper/conf/cp zoo_sample.cfg zoo.cfg #修改文件名mkdir -p /export/server/zookeeper/zkdatas/vim zoo.cfg 修改如下： 12345678910#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 2.添加myid的配置 12echo 1 &gt; /export/server/zookeeper/zkdatas/myid #在node1主机的/export/server/zookeeper/zkdatas/这个路径下创建一个文件，文件名为myid ,文件内容为1 分发myid 123cd /export/server/scp -r /export/server/zookeeper-3.4.6/ node2:$PWD scp -r /export/server/zookeeper-3.4.6/ node3:$PWD 3.在node2,node3修改myid的值，并建立软连接node2： 123cd /export/server/ln -s zookeeper-3.4.6/ zookeeper echo 2 &gt; /export/server/zookeeper/zkdatas/myid #node2 myid值为2 node3： 123cd /export/server/ln -s zookeeper-3.4.6/ zookeeper echo 3 &gt; /export/server/zookeeper/zkdatas/myid #node3 myid值为3 （3）启动zookeeper（三台） 12/export/server/zookeeper/bin/zkServer.sh start #启动 /export/server/zookeeper/bin/zkServer.sh status #查看启动状态","categories":[],"tags":[]}],"categories":[],"tags":[]}