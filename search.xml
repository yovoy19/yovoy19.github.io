<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>kafka Eagle运维监控</title>
      <link href="/2022/06/18/Eagle%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7/"/>
      <url>/2022/06/18/Eagle%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<p>kafka 自身并没有继承监控管理系统, 因此对 kafka 的监控管理比较不便, 好在有大量的第三方监控管理系统来使用</p><div class="story post-story"><h2 id="一、安装部署kafka-Eagle"><a href="#一、安装部署kafka-Eagle" class="headerlink" title="一、安装部署kafka Eagle"></a>一、安装部署kafka Eagle</h2><p>####### 安装包下载地址：<a href="https://github.com/smartloli/kafka-eagle-bin/">https://github.com/smartloli/kafka-eagle-bin/</a></p><h6 id="1-解压、安装"><a href="#1-解压、安装" class="headerlink" title="1.解压、安装"></a>1.解压、安装</h6><h6 id="2-配置环境变量：JAVA-HOME和KE-HOME"><a href="#2-配置环境变量：JAVA-HOME和KE-HOME" class="headerlink" title="2.配置环境变量：JAVA_HOME和KE_HOME"></a>2.配置环境变量：JAVA_HOME和KE_HOME</h6><p>vi &#x2F;etc&#x2F;proflie</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/export/server/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></table></figure><h6 id="3-配置-KafkaEagle"><a href="#3-配置-KafkaEagle" class="headerlink" title="3.配置 KafkaEagle"></a>3.配置 KafkaEagle</h6><p>cd &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;conf<br>vi system-config.properties</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=node1:2181,node2:2181,node3:2181</span><br><span class="line">cluster1.kafka.eagle.broker.size=3</span><br><span class="line"></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/export/data/db/ke.db</span><br></pre></td></tr></table></figure><h6 id="4-启动Eagle"><a href="#4-启动Eagle" class="headerlink" title="4.启动Eagle"></a>4.启动Eagle</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /export/data/db  <span class="comment">##启动前需要手动创建/export/data/db目录</span></span><br><span class="line">/export/server/kafka-eagle/bin/ke.sh start  <span class="comment">#启动Eagle</span></span><br></pre></td></tr></table></figure><p>(base) [root@node1 ~]# jps<br>5840 ResourceManager<br>88385 KafkaEagle<br>9203 Master<br>6023 NodeManager<br>87817 Kafka<br>5193 NameNode<br>100348 Jps<br>2383 QuorumPeerMain<br>5407 DataNode</p></div><div class="story post-story"><h2 id="二、kafka-Eagle中的各项功能"><a href="#二、kafka-Eagle中的各项功能" class="headerlink" title="二、kafka Eagle中的各项功能"></a>二、kafka Eagle中的各项功能</h2></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kafka API 使用方法</title>
      <link href="/2022/06/10/kafka%20API%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
      <url>/2022/06/10/kafka%20API%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<div class="story post-story"><h2 id="一、生产者-API"><a href="#一、生产者-API" class="headerlink" title="一、生产者 API"></a>一、生产者 API</h2><p>一个生产者逻辑应具备以下步骤：<br>(1)配置生产者客户端参数及创建相应的生产者实例<br>(2)构建待发送的消息<br>(3)发送消息<br>(4)关闭生产者实例<br>Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p><h5 id="1、引入-maven-依赖"><a href="#1、引入-maven-依赖" class="headerlink" title="1、引入 maven 依赖"></a>1、引入 maven 依赖</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h5 id="2、采用默认分区方式将消息散列的发送到各个分区当中"><a href="#2、采用默认分区方式将消息散列的发送到各个分区当中" class="headerlink" title="2、采用默认分区方式将消息散列的发送到各个分区当中"></a>2、采用默认分区方式将消息散列的发送到各个分区当中</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"><span class="comment">#kafka 地址</span></span><br><span class="line">   private static final String SERVERS = <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>;</span><br></pre></td></tr></table></figure><p>ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，<br>0–&gt;不等响应就继续发（可靠性低），1–&gt;leader会写到本地日志后，<br>然后给响应，producer拿到响应才继续发（follwer还没同步）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">props.put(“retries”, 3); </span><br><span class="line">  <span class="comment">#失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">props.put(“batch.size”, 10); </span><br><span class="line"> <span class="comment">#数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, 10000);</span><br><span class="line"><span class="comment">#消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">props.put(<span class="string">&quot;max.request.size&quot;</span>,10);   <span class="comment">#数据发送请求的最大缓存数</span></span><br><span class="line">props.put(“buffer.memory”, 10240); </span><br><span class="line"><span class="comment">#整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端</span></span><br><span class="line"><span class="comment">#buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);  <span class="comment">#key-value序列化器</span></span><br><span class="line">props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”);</span><br><span class="line"><span class="keyword">for</span> (int i = 0; i &lt; 100; i++) </span><br><span class="line">producer.send(new ProducerRecord&lt;String,String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), <span class="string">&quot;dd:&quot;</span>+i)); //Thread.<span class="built_in">sleep</span>(1000000); producer.close(); &#125; &#125;</span><br></pre></td></tr></table></figure><h5 id="3、创建生产者实例和构建消息"><a href="#3、创建生产者实例和构建消息" class="headerlink" title="3、创建生产者实例和构建消息"></a>3、创建生产者实例和构建消息</h5><p>发送消息主要有 3 种模式：<br>（1）发后即忘( fire-and-forget)<br>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。<br>在大多数情况下,这种发送方式没有问题;<br>不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。<br>这种发送方式的性能最高,可靠性最差。<br>Future<RecordMetadata> send &#x3D; producer.send(rcd);<br>（2）同步发送(sync )</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">producer.send(rcd).get(); </span><br><span class="line">&#125; catch (Exception e) &#123; </span><br><span class="line">e.printStackTrace();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>0.8.x 前,有一个参数 producer.type&#x3D;sycn|asycn 来决定生产者的发送模式;现已失效(新版中,producer 在底层只有异步)<br>（3）异步发送(async )<br>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</p></div><div class="story post-story"><h2 id="二、消费者-API"><a href="#二、消费者-API" class="headerlink" title="二、消费者 API"></a>二、消费者 API</h2><p> kafka的消费API支持订阅主题 (subscribe)和指定分区 (assign), 同时还可以通过 seek 对offset进行重置, 进行灵活的消费控制。<br>一个正常的消费逻辑需要具备以下几个步骤:<br>(1)配置消费者客户端参数<br>(2)创建相应的消费者实例;<br>(3)订阅主题;<br>(4)拉取消息并消费;<br>(5)提交消费位移 offset;<br>(6)关闭消费者实例。</p><h4 id="1、订阅主题"><a href="#1、订阅主题" class="headerlink" title="1、订阅主题"></a>1、订阅主题</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）指定集合方式订阅主题</span><br><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer subscribe(Arrays.asList(topic2))</span><br><span class="line">（2）正则方式订阅主题</span><br><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br><span class="line">利用正则表达式订阅主题,可实现动态订阅;</span><br></pre></td></tr></table></figure><p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p><h5 id="（3）assign-订阅主题"><a href="#（3）assign-订阅主题" class="headerlink" title="（3）assign 订阅主题"></a>（3）assign 订阅主题</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(new TopicPartition (<span class="string">&quot;tpc_1&quot;</span> , 0),new TopicPartition(“tpc_2”,1))) ;</span><br></pre></td></tr></table></figure><p>消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </p><p>在 KafkaConsumer 中提供了 assign() 方法来实现这些功能：<br>public void assign(Collection<TopicPartition> partitions)<br>这个方法只接受参数 partitions,用来指定需要订阅的分区集合</p><p>subscribe 与 assign 的区别</p><p>通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ;<br>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。<br>当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。<br>assign() 方法订阅分区时,是不具备消费者自动均衡的功能的;<br>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有</p><h4 id="2、取消订阅"><a href="#2、取消订阅" class="headerlink" title="2、取消订阅"></a>2、取消订阅</h4><p>可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅;<br>也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下:<br>consumer.unsubscribe(); </p><p>如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line">consumer.subscribe(new ArrayList&lt;String&gt;()) ; </span><br><span class="line">consumer.assign(new ArrayList&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure><h4 id="3、消息的消费模式"><a href="#3、消息的消费模式" class="headerlink" title="3、消息的消费模式"></a>3、消息的消费模式</h4><p>消息的消费一般有两种模式:推送模式和拉取模式。<br>推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。<br>Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。<br>对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public ConsumerRecords&lt;K, V&gt; poll(final Duration <span class="built_in">timeout</span>) </span><br></pre></td></tr></table></figure><p>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">topic partition   <span class="comment">#这两个字段分别代表消息所属主题的名称和所在分区的编号。</span></span><br><span class="line">offsset     <span class="comment">#表示消息在所属分区的偏移量。</span></span><br><span class="line">timestamp  <span class="comment">#表示时间戳,与此对应的 timestampType 表示时间戳的类型。</span></span><br><span class="line">timestampType  <span class="comment">#有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</span></span><br><span class="line">headers  <span class="comment">#表示消息的头部内容</span></span><br><span class="line">key value  <span class="comment">#分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </span></span><br><span class="line">serializedKeySize、serializedValueSize  <span class="comment">#分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </span></span><br><span class="line">checksum   <span class="comment"># CRC32 的校验值</span></span><br></pre></td></tr></table></figure><h5 id="1）均衡监听器"><a href="#1）均衡监听器" class="headerlink" title="(1）均衡监听器"></a>(1）均衡监听器</h5><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡;<br>如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p><h5 id="（2）指定位移消费"><a href="#（2）指定位移消费" class="headerlink" title="（2）指定位移消费"></a>（2）指定位移消费</h5><p>seek()方法代码：<br>public void seek(TopicPartiton partition,long offset)<br>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</p><h5 id="（3）自动位移提交"><a href="#（3）自动位移提交" class="headerlink" title="（3）自动位移提交"></a>（3）自动位移提交</h5><p>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.<br>auto.commit 参数为 true。</p><p>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。<br>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。<br>重复消费<br>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。<br>丢失消息<br>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</p><h5 id="（4）手动位移提交"><a href="#（4）手动位移提交" class="headerlink" title="（4）手动位移提交"></a>（4）手动位移提交</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">enable.auto.commit 配置为 fals ；</span><br><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br></pre></td></tr></table></figure><p>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法<br>异步提交方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public void commitAsync()</span><br><span class="line">public void commitAsync(OffsetCommitCallback ca11back)</span><br><span class="line">public void commitAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, </span><br><span class="line">OffsetCommitCallback callback)</span><br></pre></td></tr></table></figure><p>commitSync()方法相反,异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞;可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操 。异步提交以便消费者的性能得到一定的增强。 commitAsync 方法有一个不同的重载方法<br>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public void commitSync(final Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets)</span><br></pre></td></tr></table></figure><p>对于采用 commitSync()的无参方法,它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的, 如果想寻求更细粒度的、 更精准的提交, 那么就需要使用 commitSync()的另一个有参方法, 具体定义 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">其他参数</span><br><span class="line">fetch.min.bytes=1B <span class="comment">#一次拉取的最小字节数</span></span><br><span class="line">fetch.max.bytes=50M <span class="comment">#一次拉取的最大数据量</span></span><br><span class="line">fetch.max.wait.ms=500ms <span class="comment">#拉取时的最大等待时长</span></span><br><span class="line">max.partition.fetch.bytes = 1MB <span class="comment">#每个分区一次拉取的最大数据量</span></span><br><span class="line">max.poll.records=500<span class="comment">#一次拉取的最大条数</span></span><br><span class="line">connections.max.idle.ms=540000ms <span class="comment">#网络连接的最大闲置时长</span></span><br><span class="line">request.timeout.ms=30000ms   <span class="comment">#一次请求等待响应的最大超时时间consumer     #等待请求响应的最长时间</span></span><br><span class="line">metadata.max.age.ms=300000 <span class="comment">#元数据在限定时间内没有进行更新,则会被强制更新</span></span><br><span class="line">reconnect.backoff.ms=50ms <span class="comment">#尝试重新连接指定主机之前的退避时间</span></span><br><span class="line">retry.backoff.ms=100ms <span class="comment">#尝试重新拉取数据的重试间隔</span></span><br><span class="line">isolation.level=read_uncommitted <span class="comment">#隔离级别! 决定消费者能读到什么样的数据</span></span><br><span class="line">read_uncommitted:<span class="comment">#可以消费到 LSO(LastStableOffset)位置; </span></span><br><span class="line">read_committed:<span class="comment">#可以消费到 HW(High Watermark)位置</span></span><br><span class="line">max.poll.interval.ms <span class="comment">#超过时限没有发起 poll 操作,则消费组认为该消费者已离开消费组</span></span><br><span class="line">enable.auto.commit=<span class="literal">true</span> <span class="comment">#开启消费位移的自动提交</span></span><br><span class="line">auto.commit.interval.ms=5000 <span class="comment">#自动提交消费位移的时间间隔</span></span><br></pre></td></tr></table></figure><h5 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">package ccjzrgzn.kafka;</span><br><span class="line"></span><br><span class="line">        import org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line">        import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">        import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">        import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line">        import org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line">        import java.time.Duration;</span><br><span class="line">        import java.util.Arrays;</span><br><span class="line">        import java.util.Properties;</span><br><span class="line">        import java.util.concurrent.atomic.AtomicBoolean;</span><br><span class="line"></span><br><span class="line">public class ConsumerDemo &#123;</span><br><span class="line"></span><br><span class="line">    private static final String SERVERS = <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        //定义一个AtomicBoolean类型的isRunning来控制消费者拉取消息</span><br><span class="line">        AtomicBoolean isRunning = new AtomicBoolean(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        //1.参数配置</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        //key的反序列化器</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        //value的反序列化器</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line">        //服务器地址</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,SERVERS);</span><br><span class="line">        //设置自动读取的起始offset（偏移量），值可以是：earliest，latest，none</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">&quot;earliest&quot;</span>);</span><br><span class="line">        //设置自动提交offset（偏移量）</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="literal">true</span>);</span><br><span class="line">        //设置消费者组</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">&quot;b1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        //2.构建consumer实例</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        //3.订阅主题</span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>));</span><br><span class="line"></span><br><span class="line">        //4.拉取消息</span><br><span class="line">        Thread thread = new Thread(new <span class="function"><span class="title">Runnable</span></span>() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void <span class="function"><span class="title">run</span></span>() &#123;</span><br><span class="line">                <span class="keyword">while</span> (isRunning.get())&#123;</span><br><span class="line">                    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(Long.MAX_VALUE));</span><br><span class="line">                    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                        //do some process做一些处理</span><br><span class="line">                        System.out.println(record.key()+<span class="string">&quot;,&quot;</span></span><br><span class="line">                                +record.value()+<span class="string">&quot;,&quot;</span></span><br><span class="line">                                +record.topic()+<span class="string">&quot;,&quot;</span></span><br><span class="line">                                +record.partition()+<span class="string">&quot;,&quot;</span></span><br><span class="line">                                +record.offset());</span><br><span class="line">                        System.out.println(<span class="string">&quot;-------------------------分割线--------------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        thread.start();</span><br><span class="line"></span><br><span class="line">        //主线程可以去休眠60s</span><br><span class="line">        Thread.<span class="built_in">sleep</span>(60000);</span><br><span class="line"></span><br><span class="line">        //修改isRunning的值为<span class="literal">false</span></span><br><span class="line">        isRunning.<span class="built_in">set</span>(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //5.关闭consumer实例</span><br><span class="line">        consumer.close();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>kafka的消费API支持订阅主题(subscribe)和指定分区(assign), 同时还可以通过seek对offset进行重置, 进行灵活的消费控制.<br>提交offset的时候, 可以用consumer.commitSync()提交所有分区当前poll后的offset, 也可以用consumer.commitSync(offsets)来手动指定提交的offset, 当然也可以将offset存储在外部数据源, 配合seek实现 “精准一次” 的消费语义.</p></div><div class="story post-story"><h2 id="三、Topic管理-API"><a href="#三、Topic管理-API" class="headerlink" title="三、Topic管理 API"></a>三、Topic管理 API</h2><p>Kafka官方提供了两个脚本来管理topic，包括topic的增删改查。其中kafka-topics.sh负责topic的创建与删除；kafka-configs.sh脚本负责topic的修改和查询。<br>一般情况下,使用 kafka-topic.sh 来管理主题。当用到管理类功能时程序需要调用 API 方式去实现。调用 API方式实现管理主要利用 KafkaAdminClient 工具类。</p><p>创建主题:CreateTopicsResult createTopics(Collection<NewTopic>newTopics<br>删除主题:DeleteTopicsResult deleteTopics(Collection<string>topics)。<br>列出所有可用的主题:ListTopicsResultlistTopics)。<br>查看主题的信息:DescribeTopicsResult describeTopics(Collection<strina>topicNames).</p><p>查询配置信息:DescribeConfigsResult describeConfiss(Collection<ConfiaResource>resources).<br>修改配置信息:AlterConfigsResultalterConfigs(Map<ConfigResourceConfig>configs)。<br>增加分区:CreatePartitionsResult createPartitions(Map&lt;String.NewPartitions&gt;newPartitions)</p><h5 id="代码实例-1"><a href="#代码实例-1" class="headerlink" title="代码实例"></a>代码实例</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package ccjzrgzn.kafka;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.admin.*;</span><br><span class="line">import org.apache.kafka.common.KafkaFuture;</span><br><span class="line"></span><br><span class="line">import java.util.*;</span><br><span class="line">import java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line">public class KafkaAdminDemo &#123;</span><br><span class="line">    private static final String SERVERS = <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.setProperty(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, SERVERS);</span><br><span class="line"></span><br><span class="line">        //1.构建一个管理客户端的对象</span><br><span class="line">        AdminClient adminClient = KafkaAdminClient.create(props);</span><br><span class="line">        //2.列出集群中的主题信息</span><br><span class="line">        ListTopicsResult listTopicsResult = adminClient.listTopics();</span><br><span class="line">        KafkaFuture&lt;Set&lt;String&gt;&gt; names = listTopicsResult.names();</span><br><span class="line">        Set&lt;String&gt; topicNames = names.get();</span><br><span class="line">        System.out.println(topicNames);</span><br><span class="line">        //3.查看一个topic的具体信息</span><br><span class="line">        DescribeTopicsResult tpc_1 = adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_10&quot;</span>));</span><br><span class="line">        KafkaFuture&lt;Map&lt;String, TopicDescription&gt;&gt; future = tpc_1.all();</span><br><span class="line">        Map&lt;String, TopicDescription&gt; stringTopicDescriptionMap = future.get();//get()会阻塞直到拿到返回值</span><br><span class="line">        Set&lt;Map.Entry&lt;String, TopicDescription&gt;&gt; entries = stringTopicDescriptionMap.entrySet();</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, TopicDescription&gt; entry : entries) &#123;</span><br><span class="line">            System.out.println(entry.getKey());</span><br><span class="line">            TopicDescription desc = entry.getValue();</span><br><span class="line">            System.out.println(desc.name()+<span class="string">&quot;,&quot;</span>+desc.partitions());</span><br><span class="line">            System.out.println(<span class="string">&quot;--------------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">//4.创建topic</span><br><span class="line">        HashMap&lt;Integer, List&lt;Integer&gt;&gt; patitions = new HashMap&lt;&gt;();</span><br><span class="line">        patitions.put(0,Arrays.asList(0,2));</span><br><span class="line">        patitions.put(1,Arrays.asList(1,2));</span><br><span class="line">        patitions.put(2,Arrays.asList(0,1));</span><br><span class="line">        NewTopic tpc_10 = new NewTopic(<span class="string">&quot;tpc_10&quot;</span>,patitions);</span><br><span class="line">//        NewTopic tpc_10 = new NewTopic(<span class="string">&quot;tpc_10&quot;</span>,2,(short) 3);</span><br><span class="line">        adminClient.createTopics(Arrays.asList(tpc_10));</span><br><span class="line"></span><br><span class="line">        adminClient.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kafka命令行操作</title>
      <link href="/2022/06/10/kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
      <url>/2022/06/10/kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Kafka 中提供了许多命令行工具用于管理集群的变更。</p><table><thead><tr><th>kafka-configs.sh</th><th>用于配置管理</th></tr></thead><tbody><tr><td>kafka-console-consumer.sh</td><td>用于消费消息</td></tr><tr><td>kafka-console-producer.sh</td><td>用于生产消息</td></tr><tr><td>kafka-consumer-perf-test.sh</td><td>用于测试消费性能</td></tr><tr><td>kafka-topics.sh</td><td>用于管理主题</td></tr><tr><td>kafka-dump-log.sh</td><td>用于查看日志内容</td></tr><tr><td>kafka-server-stop.sh</td><td>用于关闭Kafka 服务</td></tr><tr><td>kafka-preferred-replica-election.sh</td><td>用于优先副本的选举</td></tr><tr><td>kafka-server-start.sh</td><td>用于启动Kafka服务</td></tr><tr><td>kafka-producer-perf-test.sh</td><td>用于测试生产性能</td></tr><tr><td>kafka-reassign-partitions.sh</td><td>查看帮助</td></tr></tbody></table><p>1、查看topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topic.sh --list --zookeeper node1:2181 __consumer_offsets</span><br></pre></td></tr></table></figure><p>2、创建topic<br>（1）.不指定分区创建topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 </span><br><span class="line">--create --replication-factor 3 --partitions 3 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p>（2）手动指定副本的存储位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line"></span><br><span class="line">--replication-factor   <span class="comment">#副本数量</span></span><br><span class="line">--partitions   <span class="comment">#分区数量</span></span><br><span class="line">--topic      <span class="comment">#topic 名称</span></span><br></pre></td></tr></table></figure><p>注：该方式下,命令会自动判断所要创建的 topic 的分区数及副本数<br>3、删除 topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_6 --zookeeper node1：2181</span><br></pre></td></tr></table></figure><p>（异步线程去删除）删除 topic,需要一个参数处于启用状态:<br> delete.topic.enable &#x3D; true,否则删不掉<br>使用 kafka-topics .sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的&#x2F;admin&#x2F;delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。<br>与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。<br>4、查看 topic<br>（1）列出当前系统中的所有 topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 –list</span><br></pre></td></tr></table></figure><p>（2）查看 topic 详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --topic tpc_1   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181 </span><br></pre></td></tr></table></figure><p>可以看到topic 的分区数量, 以及每个分区的副本数量,以及每个副本所在的 broker 节点,以及每个分区的 leader 副本所在 broker 节点,以及每个分区的 ISR 副本列表;<br> ISR: in sync replicas    #同步副本(当然也包含 leader 自身)<br>OSR:out of sync replicas   #失去同步的副本(数据与 leader 之间的差距超过配置的阈值)<br>5、增加分区数<br>Kafka 只支持增加分区,不支持减少分区，原因是减少分区,代价太大(数据的转移,日志段拼接合并)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></figure><p>6、动态配置 topic 参数<br>通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数。<br>（1）添加、修改配置参数<br>（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 </span><br><span class="line">--entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip</span><br></pre></td></tr></table></figure><p>（2）删除配置参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper node1:2181</span><br><span class="line">--entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></figure><p>7、生产者写入数据、消费者拉取数据<br>（1）生产者：kafka-console-producer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh </span><br><span class="line">--broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1  </span><br></pre></td></tr></table></figure><p>（2）消费者：kafka-console-producer</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从头开始</span></span><br><span class="line">bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning</span><br><span class="line"><span class="comment"># 指定要消费的分区,和要消费的起始 offset </span></span><br><span class="line">bin/kafka-console-consumer.sh </span><br><span class="line">--bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>kafka环境配置</title>
      <link href="/2022/06/10/kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2022/06/10/kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Kafka 是一种分布式的，基于发布 &#x2F; 订阅的消息系统。主要设计目标如下：</p><p>（1）以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。<br>（2）高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。<br>（3）支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。<br>（4）同时支持离线数据处理和实时数据处理。<br>Scale out：支持在线水平扩展</p><h3 id="一、基础配置"><a href="#一、基础配置" class="headerlink" title="一、基础配置"></a>一、基础配置</h3><p>1.导入三台已配置好zookeeper组件以及免密互通的虚拟机</p><h3 id="二、安装配置kafka"><a href="#二、安装配置kafka" class="headerlink" title="二、安装配置kafka"></a>二、安装配置kafka</h3><p>2.在master主机上，上传安装包，解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-2.0.0.tgz</span><br><span class="line"><span class="built_in">ln</span> -s kafka_2.11-2.0.0/ kafka <span class="comment">#建立软连接</span></span><br></pre></td></tr></table></figure><p>3.修改配置文件 server.properties</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi/export/server/kafka/config/server.properties</span><br></pre></td></tr></table></figure><p>修改内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#listeners=PLAINTEXT://:9092 取消注释，内容改为：</span></span><br><span class="line">listeners=PLAINTEXT://master:9092  <span class="comment">#PLAINTEXT为通信使用明文（加密ssl）</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.dirs=/tmp/kafka-logs为默认日志文件存储的位置改为</span><br><span class="line">log.dirs=/export/server/data/kafka-logs</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）</span></span><br><span class="line">interval.messages interval.ms</span><br><span class="line">168/24=7，1073741824/1024=1GB      <span class="comment">#数据保留策略</span></span><br><span class="line">300000ms = 300s = 5min         <span class="comment">#超过了删掉</span></span><br><span class="line">（最后修改时间还是创建时间--&gt;日志段中最晚的一条消息，维护这个最大的时间戳--&gt;用户无法 干预）</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=localhost:2181 修改为</span><br><span class="line">zookeeper.connect=master:2181,slave1:2181,slave2:2181</span><br><span class="line">group.initial.rebalance.delay.ms=0 修改为 group.initial.rebalance.delay.ms=3000</span><br></pre></td></tr></table></figure><p>4.分发配置文件<br>给node2，node3分发kafka</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/ </span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node2:<span class="variable">$PWD</span> </span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node3:<span class="variable">$PWD</span> </span><br></pre></td></tr></table></figure><p>5.配置环境变量（三台）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/export/server/kafka</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>6.node2,node3修改配置文件server.properties：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi server.properties</span><br><span class="line">listeners=PLAINTEXT://master:9092修改listeners=PLAINTEXT://node2:9092 </span><br><span class="line">同理node3 同样操作</span><br></pre></td></tr></table></figure><p>7.验证kafka</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（基于zookeeper启动）</span><br><span class="line">kafka-server-start.sh -daemon /export/ <span class="comment">#启动kafka</span></span><br><span class="line">kafka-server-stop.sh stop  <span class="comment">#关闭kafka</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark HA &amp; Yarn配置</title>
      <link href="/2022/05/24/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/"/>
      <url>/2022/05/24/Spark%20HA%20&amp;%20Yarn%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Spark支持ZooKeeper来实现 HA,<br>on yarn:运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算</p><div class="story post-story"><h2 id="一、Spark（HA）"><a href="#一、Spark（HA）" class="headerlink" title="一、Spark（HA）"></a>一、Spark（HA）</h2><p>（1）修改spark-env.sh配置文件<br>删除<code>SPARK_MASTER_HOST=node1</code>不固定master节点<br>增加内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 设置历史服务器</span></span><br><span class="line"><span class="comment"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span></span><br><span class="line">SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line"><span class="comment"># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="comment"># 指定Zookeeper的连接地址</span></span><br><span class="line"><span class="comment"># 指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure><p>(2)将spark-env.sh分发到node2和node3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf</span><br></pre></td></tr></table></figure><p>(3)启动集群<br>1.关闭当前StandAlone集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-all.sh</span><br></pre></td></tr></table></figure><p>2.在node1上 启动一个master 和全部worker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/sbin/start-all.sh</span><br><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>3.在node2上启动一个备用的master进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure><p>(base) [root@node1 ~]# jps<br>3985 Worker<br>3749 Master<br>4300 Jps<br>2383 QuorumPeerMain<br>4.查看状态：①node1:8080端口状态为ALIVE<br>在node2上是备用master，当node1启用master时node2状态为standby<br>(8080端口可能会发生顺延)<br>5.关闭node1的master进程，node2master启用其状态切换为ALIVE<br>(4)测试：提交一个spark任务到当前<code>ALIVE</code>master上:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://node1:7077 </span><br><span class="line">/export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></figure><p>提交后，将<code>ALIVE</code>master kill掉，不会影响程序<br>当新的master（即使用node2备用master）接收集群后, 程序继续运行, 正常得到结果<br>HA模式下，主备切换不会影响正在运行的程序</p></div><div class="story post-story"><h2 id="二、Spark-Yarn"><a href="#二、Spark-Yarn" class="headerlink" title="二、Spark(Yarn)"></a>二、Spark(Yarn)</h2><p>yarn-cluster：适用于生产环境<br>yarn-client：适用于交互、调试，希望立即看到app的输出<br>在spark-env.sh 以及 环境变量配置文件中即可<br>(1)连接到YARN中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure><p>注意: 交互式环境 pyspark  和 spark-shell  无法运行 cluster模式<br>(2)client 模式测试<br>测试yarn提交spark任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/bin/spark-submit --master yarn --deploy-mode cluster \ </span><br><span class="line">--driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 \</span><br><span class="line">/export/server/spark/examples/src/main/python/pi.py 3 </span><br></pre></td></tr></table></figure><p>(3)cluster 模式测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/bin/spark-submit --master yarn --deploy-mode cluster \</span><br><span class="line">--driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 \</span><br><span class="line">/export/server/spark/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure><p>(4)通过web UI查看任务运行状态<br><a href="http://master:8080/">http://master:8080/</a></p></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spark local &amp; stand-alone配置</title>
      <link href="/2022/05/24/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/"/>
      <url>/2022/05/24/Spark%20local&amp;%20stand-alone%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Spark 有多种运行模式， Spark 支持本地运行模式（Local 模式）、独立运行模式（Standalone 模式）、YARN（Yet Another Resource Negotiator）<br>local(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程;<br>standalone(集群模式)：典型的Mater&#x2F;slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA</p><div class="story post-story"><h2 id="一、spark（local）"><a href="#一、spark（local）" class="headerlink" title="一、spark（local）"></a>一、spark（local）</h2><p>###（一）Anaconda On Linux安装（单机服务器脚本安装）<br>（1）上传解压安装包，安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>一次回车+5次空格<br>Do you accept the license terms? [yes&#x2F;no]&gt;&gt;&gt;  yes<br>Anaconda3 will now be installed into this location:<br>&#x2F;root&#x2F;anaconda3<br>    - Press ENTER to confirm the location<br>    - Press CTRL -C to abort the installation<br>    - Or specify a different location below<br>[&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 (Anaconda3安装位置，推荐在&#x2F;export&#x2F;server&#x2F;anaconda3)<br>by running conda init? [yes&#x2F;no] &gt;&gt;&gt; yes （是否初始化，输yes）<br>安装完成，重新连接<br>    (base) [root@node1 ~]#<br>出现bash安装完成<br>（2）创建虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pyspark 基于 python3.8 </span></span><br><span class="line">conda create -n pyspark python=3.8</span><br><span class="line"><span class="comment"># 切换到虚拟环境内</span></span><br><span class="line">conda activate pyspark</span><br><span class="line"><span class="comment"># 在虚拟环境内安装包</span></span><br><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple </span><br></pre></td></tr></table></figure><p>(pyspark) [root@node1 ~]#<br>(3)安装spark<br>1.上传，解压，压缩包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure><p>2.建立软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>3.添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="comment">#SPARK_HOME</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/server/spark</span><br><span class="line"><span class="comment">#HADOOP_CONF_DIR</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="comment">#PYSPARK_PYTHON</span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure><p>重新加载环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>4.进入pyspark界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line"> ./pyspark</span><br></pre></td></tr></table></figure><p> (base) [root@node1 bin]#  .&#x2F;pyspark<br>Python 3.8.13 (default, Mar 28 2022, 11:38:47)<br>[GCC 7.5.0] :: Anaconda, Inc. on linux<br>Type “help”, “copyright”, “credits” or “license” for more information.<br>22&#x2F;05&#x2F;24 19:26:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Welcome to<br>      ____              __<br>     &#x2F; <strong>&#x2F;</strong>  ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>    <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F;  ‘</em>&#x2F;<br>   &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\   version 3.2.0<br>      &#x2F;</em>&#x2F;</p><p>Using Python version 3.8.13 (default, Mar 28 2022 11:38:47)<br>Spark context Web UI available at <a href="http://node1:4040/">http://node1:4040</a><br>Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1653391619858).<br>SparkSession available as ‘spark’.</p><p> 5.浏览器访问验证<br> <a href="http://master:4040/">http://master:4040/</a></p></div><div class="story post-story"><h2 id="二、Spark-stand-alone"><a href="#二、Spark-stand-alone" class="headerlink" title="二、Spark(stand-alone)"></a>二、Spark(stand-alone)</h2><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境<br>（1）master 节点修改配置文件<br>进入文件路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/conf</span><br></pre></td></tr></table></figure><p>1.将文件 workers.template 改名为 workers，并配置文件内容<br>将里面的localhost删除，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> workers.template  workers</span><br><span class="line">vi workers</span><br><span class="line">删除localhost 改为</span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3 </span><br></pre></td></tr></table></figure><p>2.将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br></pre></td></tr></table></figure><p>添加以下配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">## 指定spark老大Master的IP和提交任务的通信端口</span></span><br><span class="line"><span class="comment"># 告知Spark的master运行在哪个机器上</span></span><br><span class="line"><span class="comment">#export SPARK_MASTER_HOST=node1</span></span><br><span class="line"><span class="comment"># 告知sparkmaster的通讯端口</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="comment"># 告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="comment"># worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="comment"># worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="comment"># worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3.修改spark-defaults.conf</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi spark-defaults.conf</span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment"># spark.master                     spark://master:7077</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>               hdfs://node1:8020/sparklog/</span><br><span class="line"><span class="comment"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span></span><br><span class="line"><span class="comment"># spark.driver.memory              5g</span></span><br><span class="line"><span class="comment"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span></span><br><span class="line">spark.eventLog.compress         <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>4.在HDFS上创建程序运行历史记录存放的文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> /sparklog</span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /sparklog</span><br></pre></td></tr></table></figure><p>5.配置 log4j.properties 文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootCategory=INFO, console 改为 log4j.rootCategory=WARN, console </span><br></pre></td></tr></table></figure><p>即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息<br>6.master 节点分发 spark 安装文件夹 到 node2 和 node3 上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:<span class="variable">$PWD</span></span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p>(2)在node2 和 node3 上做软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/sprk-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>(3)验证<br>重新加载环境变量，启动Spark的Master和Worker进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"><span class="built_in">cd</span> /export/server/spark/sbin</span><br><span class="line">./start-history-server.sh</span><br></pre></td></tr></table></figure><p>(base) [root@node1 ~]# jps<br>3985 Worker<br>3749 Master<br>4300 Jps<br>2383 QuorumPeerMain<br>(4)访问 WebUI界面: http:&#x2F;&#x2F;主节点ip:8080&#x2F;<br>默认端口master我们设置到了8080<br>如果端口被占用, 会顺延到8081 …;8082… 8083… 直到申请到端口为止<br>可以在日志中查看, 具体顺延到哪个端口上:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Service <span class="string">&#x27;MasterUI&#x27;</span> could not <span class="built_in">bind</span> on port 8080. Attempting port 8081.</span><br></pre></td></tr></table></figure><p>1.连接到standalone集群<br>测试代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line"> ./pyspark</span><br><span class="line">sc.parallelize(Array(1,2,3,4,5)).map(x=&gt; x + 1).collect()</span><br></pre></td></tr></table></figure><p>sc.parallelize(Array(1,2,3,4,5)).map(x&#x3D;&gt; x + 1).collect()<br>[2,3,4,5,6]<br>查看历史服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://node1:7077 </span><br><span class="line">/export/server/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></figure><p>历史服务器的默认端口是: 18080<br>http:&#x2F;&#x2F;主节点ip:18080&#x2F;</p></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spark基础环境配置</title>
      <link href="/2022/05/21/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83/"/>
      <url>/2022/05/21/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<p>spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载</p><div class="story post-story"><h2 id="一、基础环境"><a href="#一、基础环境" class="headerlink" title="一、基础环境"></a>一、基础环境</h2><p>1.导入三台虚拟机，配置如下</p><table><thead><tr><th>主机名</th><th>node1</th><th>node2</th><th>node3</th></tr></thead><tbody><tr><td>IP</td><td>192.168.231.151</td><td>192.168.231.152</td><td>192.168.231.153</td></tr><tr><td>用户名、密码</td><td>root&#x2F;123456</td><td>root&#x2F;123456</td><td>root&#x2F;123456</td></tr></tbody></table><p>2.集群角色规划</p><table><thead><tr><th>服务器</th><th>运行角色</th></tr></thead><tbody><tr><td>node1.itcast.cn</td><td>Namenode(主角色)，Datanode(从角色)，Resourcemanager(主角色)，Nodemanager(从角色)，master，follower，worker，QuorumPeerMain,sparksubmit</td></tr><tr><td>node2.itcast.cn</td><td>Secondarynamenode(主角色辅助角色)，datanode(从角色)，Nodemanager(从角色)，worker，leader</td></tr><tr><td>node2.itcast.cn</td><td>Datanode(从角色)，nodemanager(从角色)，worker，follower</td></tr></tbody></table><h3 id="（一）主机hosts映射（三台）："><a href="#（一）主机hosts映射（三台）：" class="headerlink" title="（一）主机hosts映射（三台）："></a>（一）主机hosts映射（三台）：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]<span class="comment"># vi /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.231.151 node1 node1.itcast.cn</span><br><span class="line">192.168.231.152 node2 node1.itcast.cn</span><br><span class="line">192.168.231.153 node3 node1.itcast.cn</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a><br>（1）关闭防火墙</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure><p>如下：<br>[root@node1 ~]# systemctl status firewalld.service<br>● firewalld.service - firewalld - dynamic firewall daemon<br>   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;firewalld.service; disabled; vendor preset: enabled)<br>   Active: inactive (dead)<br>     Docs: man:firewalld(1)<br>More info: <a href="https://hexo.io/docs/server.html">Server</a><br>（2）同步时间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp5.aliyun.com</span><br></pre></td></tr></table></figure><p>[root@node1 ~]# ntpdate ntp5.aliyun.com<br>24 May 15:40:06 ntpdate[6487]: step time server 203.107.6.88 offset 0.770984 sec<br>You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</p><h3 id="二-配置ssh免密登录（三台）"><a href="#二-配置ssh免密登录（三台）" class="headerlink" title="(二)配置ssh免密登录（三台）"></a>(二)配置ssh免密登录（三台）</h3><p>node1免密登录2、3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa(4次回车)</span><br></pre></td></tr></table></figure><p>[root@node1 ~]# ssh-keygen -t rsa<br>Generating public&#x2F;private rsa key pair.<br>Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):<br>&#x2F;root&#x2F;.ssh&#x2F;id_rsa already exists.<br>Overwrite (y&#x2F;n)? y<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.<br>Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub.<br>The key fingerprint is:<br>SHA256:Um11dr+jE0C5kCbEYyoKqZpQBqMuqSF4Pq6WpnRGtg4 <a href="mailto:&#114;&#111;&#x6f;&#x74;&#x40;&#110;&#111;&#100;&#101;&#x31;&#x2e;&#x69;&#x74;&#99;&#x61;&#x73;&#116;&#x2e;&#x63;&#110;">&#114;&#111;&#x6f;&#x74;&#x40;&#110;&#111;&#100;&#101;&#x31;&#x2e;&#x69;&#x74;&#99;&#x61;&#x73;&#116;&#x2e;&#x63;&#110;</a><br>The key’s randomart image is:<br>+—[RSA 2048]—-+<br>|      o.  ..o o .|<br>|o      &#x3D; &#x3D;.o o ..|<br>|.+    o &#x3D; +..   .|<br>|+ o. . . . ..   .|<br>|&#x3D;+.o. . S    . o |<br>|Oo+ .  .      o .|<br>|*E.+         o   |<br>|&#x3D;+B           .  |<br>|*o.o             |<br>+—-[SHA256]—–+<br>You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node2 （输入node2密码）</span><br><span class="line">ssh-copy-id node3  （node3密码）</span><br></pre></td></tr></table></figure><p>(base) [root@node1 ~]# ssh-copy-id node2<br>&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: Source of key(s) to be installed: “&#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub”<br>&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br>&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: 1 key(s) remain to be installed – if you are prompted now it is to install the new keys<br>root@node2’s password: (输入node2密码)</p><p>Number of key(s) added: 1</p><p>Now try logging into the machine, with:   “ssh ‘node2’”<br>and check to make sure that only the key(s) you wanted were added.</p><p>You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root<br>验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh node2</span><br><span class="line">ssh node3</span><br></pre></td></tr></table></figure><p>(base) [root@node1 ~]# ssh node2<br>Last login: Fri May 13 08:33:21 2022 from 192.168.231.1</p><p>同理，node2，node3也一样。</p></div><div class="story post-story"><h2 id="二、安装配置JDK"><a href="#二、安装配置JDK" class="headerlink" title="二、安装配置JDK"></a>二、安装配置JDK</h2><p>(1)编译环境软件安装目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /export/servers</span><br></pre></td></tr></table></figure><p>(2)上传、解压、安装<a href="https://www.oracle.com/java/technologies/downloads/#java8">jdk</a><br>解压至&#x2F;export&#x2F;severs、</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /export/servers/jdk-8u241-linux-x64.tar.gr -C /export/servers/</span><br></pre></td></tr></table></figure><p>JDK安装目录重命名为jdk</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> /export/servers/jdk1.8.0_241/ jdk</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a><br>(3)配置环境变量<br>添加jdk配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="comment">#JDK</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br></pre></td></tr></table></figure><p>(4)重新加载环境变量并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><p>[root@node1 ~]# java -version<br>java version “1.8.0_241”<br>Java(TM) SE Runtime Environment (build 1.8.0_241-b07)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)<br>(5)分发JDK相关文件到node2，node3<br>1.jdk相关文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/servers/jdk1.8.0_171/ root@node2:/export/servers/ </span><br><span class="line">scp -r /export/servers/jdk1.8.0_171/ root@node3:/export/servers/</span><br></pre></td></tr></table></figure><p>2.系统环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /etc/profile root@node2:/etc/profile </span><br><span class="line">scp -r /etc/profile root@node3:/etc/profile </span><br></pre></td></tr></table></figure><p>3.分别在node2、3重新使环境变量生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></div><div class="story post-story"><h2 id="三、Hadoop集群的安装和配置"><a href="#三、Hadoop集群的安装和配置" class="headerlink" title="三、Hadoop集群的安装和配置"></a>三、Hadoop集群的安装和配置</h2><p>(1)上传 <a href="https://hadoop.apache.org/releases.html">Hadoop</a> 安装包并解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</span><br></pre></td></tr></table></figure><p>(2)修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/hadoop-3.3.0/etc/hadoop</span><br></pre></td></tr></table></figure><p>1.hadoop-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line"></span><br><span class="line"><span class="comment">#Hadoop各个组件启动运行身份</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>2.core-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://node1:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 整合hive 用户代理设置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 文件系统垃圾桶保存时间 单位：分 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>3.hdfs-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;node2:9868&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>4.mapred-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 <span class="built_in">local</span>本地模式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- MR程序历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>5.yarn-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;node1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 开启日志聚集 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 设置yarn历史服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 日志保存的时间 7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>(3)将hadoop添加到环境变量(三台)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>重新加载环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>(4)向node2、3 分发hadoop目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/hadoop-3.3.0/ root@node2:/export/server/hadoop-3.3.0/</span><br><span class="line">scp -r /export/server/hadoop-3.3.0/ root@node3:/export/server/hadoop-3.3.0/</span><br></pre></td></tr></table></figure><p>(5)在node1格式化hdfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>(6)启动hadoop集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>（7）jps查看进程：<br>[root@node2 ~]# jps<br>5697 Jps<br>4867 DataNode<br>2183 QuorumPeerMain<br>5065 NodeManager<br>4973 SecondaryNameNode<br>Web UI页面验证：<br>yarn: http:&#x2F;&#x2F;主节点ip:9868&#x2F;<br>hdfs: http:&#x2F;&#x2F;主节点ip:8088&#x2F;<br>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p></div><div class="story post-story"><h2 id="四、Zookeeper集群安装"><a href="#四、Zookeeper集群安装" class="headerlink" title="四、Zookeeper集群安装"></a>四、Zookeeper集群安装</h2><p>（1）上传、解压、安装<a href="https://zookeeper.apache.org/releases.html">zookeeper</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /export/servers/zookeeper-3.4.6.tar.gr -C /export/servers/</span><br></pre></td></tr></table></figure><p>建立软连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s zookeeper/zookeeper-3.4.6</span><br></pre></td></tr></table></figure><p>（2）修改配置文件<br>1.zoo.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/conf/</span><br><span class="line"><span class="built_in">cp</span> zoo_sample.cfg zoo.cfg   <span class="comment">#修改文件名</span></span><br><span class="line"><span class="built_in">mkdir</span> -p /export/server/zookeeper/zkdatas/</span><br><span class="line">vim  zoo.cfg</span><br></pre></td></tr></table></figure><p>修改如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="comment"># 集群中服务器地址</span></span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br><span class="line">server.3=node3:2888:3888</span><br></pre></td></tr></table></figure><p>2.添加myid的配置<br>在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，<br>文件名为myid ,文件内容为1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /export/server/zookeeper/zkdatas/</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /export/server/zookeeper/zkdatas/myid  </span><br></pre></td></tr></table></figure><p>分发myid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node2:<span class="variable">$PWD</span> </span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><p>3.在node2,node3修改myid的值，并建立软连接<br>node2：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper </span><br><span class="line"><span class="built_in">echo</span> 2 &gt; /export/server/zookeeper/zkdatas/myid  <span class="comment">#node2 myid值为2</span></span><br></pre></td></tr></table></figure><p>node3：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper </span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /export/server/zookeeper/zkdatas/myid  <span class="comment">#node3 myid值为3</span></span><br></pre></td></tr></table></figure><p>(3)添加环境变量(三台)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="comment">#ZOOKEEPER_HOME</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>重新加载环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>（4）启动zookeeper（三台）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/export/server/zookeeper/bin/zkServer.sh start    <span class="comment">#启动 </span></span><br><span class="line">/export/server/zookeeper/bin/zkServer.sh  status  <span class="comment">#查看启动状态</span></span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
