{"meta":{"title":"Hexo","subtitle":"","description":"This is yovoy_blog","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"spark local & stand-alone配置","slug":"Spark local& stand-alone配置","date":"2022-05-24T10:27:19.339Z","updated":"2022-05-24T15:22:11.735Z","comments":true,"path":"2022/05/24/Spark local& stand-alone配置/","link":"","permalink":"http://example.com/2022/05/24/Spark%20local&%20stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark 有多种运行模式， Spark 支持本地运行模式（Local 模式）、独立运行模式（Standalone 模式）、YARN（Yet Another Resource Negotiator）local(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程;standalone(集群模式)：典型的Mater&#x2F;slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA 一、spark（local）###（一）Anaconda On Linux安装（单机服务器脚本安装）（1）上传解压安装包，安装 1sh./Anaconda3-2021.05-Linux-x86_64.sh 一次回车+5次空格Do you accept the license terms? [yes&#x2F;no]&gt;&gt;&gt; yesAnaconda3 will now be installed into this location:&#x2F;root&#x2F;anaconda3 - Press ENTER to confirm the location - Press CTRL -C to abort the installation - Or specify a different location below[&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 (Anaconda3安装位置，推荐在&#x2F;export&#x2F;server&#x2F;anaconda3)by running conda init? [yes&#x2F;no] &gt;&gt;&gt; yes （是否初始化，输yes）安装完成，重新连接 (base) [root@node1 ~]#出现bash安装完成（2）创建虚拟环境 123456#pyspark 基于 python3.8 conda create -n pyspark python=3.8# 切换到虚拟环境内conda activate pyspark# 在虚拟环境内安装包pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple (pyspark) [root@node1 ~]#(3)安装spark1.上传，解压，压缩包 1tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 2.建立软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 3.添加环境变量 1234567vi /etc/profile#SPARK_HOMEexport SPARK_HOME=/export/server/spark#HADOOP_CONF_DIRexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop#PYSPARK_PYTHONexport PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 重新加载环境变量： 1source /etc/profile 4.进入pyspark界面 12cd /export/server/anaconda3/envs/pyspark/bin/ ./pyspark (base) [root@node1 bin]# .&#x2F;pysparkPython 3.8.13 (default, Mar 28 2022, 11:38:47)[GCC 7.5.0] :: Anaconda, Inc. on linuxType “help”, “copyright”, “credits” or “license” for more information.22&#x2F;05&#x2F;24 19:26:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicableWelcome to ____ __ &#x2F; &#x2F; ___ _&#x2F; &#x2F; \\ / _ / _ &#96;&#x2F; _&#x2F; ‘&#x2F; &#x2F;_ &#x2F; .&#x2F;_,&#x2F;&#x2F; &#x2F;&#x2F;_\\ version 3.2.0 &#x2F;&#x2F; Using Python version 3.8.13 (default, Mar 28 2022 11:38:47)Spark context Web UI available at http://node1:4040Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1653391619858).SparkSession available as ‘spark’. 5.浏览器访问验证 http://master:4040/ 二、Spark(stand-alone)Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境（1）master 节点修改配置文件进入文件路径 1cd /export/server/spark/conf 1.将文件 workers.template 改名为 workers，并配置文件内容将里面的localhost删除， 123456mv workers.template workersvi workers删除localhost 改为node1node2node3 2.将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 12mv spark-env.sh.template spark-env.shvi spark-env.sh 添加以下配置 12345678910111213141516171819202122232425## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上#export SPARK_MASTER_HOST=node1# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现# 指定Zookeeper的连接地址# 指定在Zookeeper中注册临时节点的路径 3.修改spark-defaults.conf 123456789vi spark-defaults.conf# Example:# spark.master spark://master:7077spark.eventLog.enabled truespark.eventLog.dir hdfs://node1:8020/sparklog/# spark.serializer org.apache.spark.serializer.KryoSerializer# spark.driver.memory 5g# spark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;spark.eventLog.compress true 4.在HDFS上创建程序运行历史记录存放的文件夹 12hadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog 5.配置 log4j.properties 文件 1log4j.rootCategory=INFO, console 改为 log4j.rootCategory=WARN, console 即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息6.master 节点分发 spark 安装文件夹 到 node2 和 node3 上 12scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD (2)在node2 和 node3 上做软连接 1ln -s /export/server/sprk-3.2.0-bin-hadoop3.2 /export/server/spark (3)验证重新加载环境变量，启动Spark的Master和Worker进程 12345678source /etc/profilecd /export/server/spark/sbin./start-history-server.sh(base) [root@node1 ~]# jps3985 Worker3749 Master4300 Jps2383 QuorumPeerMain (4)访问 WebUI界面: http:&#x2F;&#x2F;主节点ip:8080&#x2F;默认端口master我们设置到了8080如果端口被占用, 会顺延到8081 …;8082… 8083… 直到申请到端口为止可以在日志中查看, 具体顺延到哪个端口上: 1Service &#x27;MasterUI&#x27; could not bind on port 8080. Attempting port 8081. 1.连接到standalone集群测试代码 123cd /export/server/anaconda3/envs/pyspark/bin/ ./pysparksc.parallelize(Array(1,2,3,4,5)).map(x=&gt; x + 1).collect() sc.parallelize(Array(1,2,3,4,5)).map(x&#x3D;&gt; x + 1).collect()[2,3,4,5,6]查看历史服务器 12bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100 历史服务器的默认端口是: 18080http:&#x2F;&#x2F;主节点ip:18080&#x2F;","categories":[],"tags":[]},{"title":"spark基础环境配置","slug":"Spark基础环境","date":"2022-05-21T09:45:28.537Z","updated":"2022-05-24T13:29:37.152Z","comments":true,"path":"2022/05/21/Spark基础环境/","link":"","permalink":"http://example.com/2022/05/21/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83/","excerpt":"","text":"spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载 一、基础环境1.导入三台虚拟机，配置如下 主机名 node1 node2 node3 IP 192.168.231.151 192.168.231.152 192.168.231.153 用户名、密码 root&#x2F;123456 root&#x2F;123456 root&#x2F;123456 2.集群角色规划 服务器 运行角色 node1.itcast.cn Namenode(主角色)，Datanode(从角色)，Resourcemanager(主角色)，Nodemanager(从角色)，master，follower，worker，QuorumPeerMain,sparksubmit node2.itcast.cn Secondarynamenode(主角色辅助角色)，datanode(从角色)，Nodemanager(从角色)，worker，leader node2.itcast.cn Datanode(从角色)，nodemanager(从角色)，worker，follower （一）主机hosts映射（三台）：1234567(base) [root@node1 ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.231.151 node1 node1.itcast.cn192.168.231.152 node2 node1.itcast.cn192.168.231.153 node3 node1.itcast.cn More info: Writing（1）关闭防火墙 1systemctl status firewalld.service 如下：[root@node1 ~]# systemctl status firewalld.service● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:firewalld(1)More info: Server（2）同步时间 1ntpdate ntp5.aliyun.com [root@node1 ~]# ntpdate ntp5.aliyun.com24 May 15:40:06 ntpdate[6487]: step time server 203.107.6.88 offset 0.770984 secYou have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root (二)配置ssh免密登录（三台）node1免密登录2、3 1ssh-keygen -t rsa(4次回车) [root@node1 ~]# ssh-keygen -t rsaGenerating public&#x2F;private rsa key pair.Enter file in which to save the key (&#x2F;root&#x2F;.ssh&#x2F;id_rsa):&#x2F;root&#x2F;.ssh&#x2F;id_rsa already exists.Overwrite (y&#x2F;n)? yEnter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.Your public key has been saved in &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub.The key fingerprint is:SHA256:Um11dr+jE0C5kCbEYyoKqZpQBqMuqSF4Pq6WpnRGtg4 &#114;&#111;&#x6f;&#116;&#x40;&#110;&#x6f;&#x64;&#101;&#49;&#x2e;&#105;&#x74;&#x63;&#97;&#x73;&#x74;&#46;&#99;&#110;The key’s randomart image is:+—[RSA 2048]—-+| o. ..o o .||o &#x3D; &#x3D;.o o ..||.+ o &#x3D; +.. .||+ o. . . . .. .||&#x3D;+.o. . S . o ||Oo+ . . o .||*E.+ o ||&#x3D;+B . ||*o.o |+—-[SHA256]—–+You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root 12ssh-copy-id node2 （输入node2密码）ssh-copy-id node3 （node3密码） (base) [root@node1 ~]# ssh-copy-id node2&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: Source of key(s) to be installed: “&#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub”&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: 1 key(s) remain to be installed – if you are prompted now it is to install the new keysroot@node2’s password: (输入node2密码) Number of key(s) added: 1 Now try logging into the machine, with: “ssh ‘node2’”and check to make sure that only the key(s) you wanted were added. You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root验证： 12ssh node2ssh node3 (base) [root@node1 ~]# ssh node2Last login: Fri May 13 08:33:21 2022 from 192.168.231.1 同理，node2，node3也一样。 二、安装配置JDK(1)编译环境软件安装目录 1mkdir -p /export/servers (2)上传、解压、安装jdk解压至&#x2F;export&#x2F;severs、 1tar -zxvf /export/servers/jdk-8u241-linux-x64.tar.gr -C /export/servers/ JDK安装目录重命名为jdk 1mv /export/servers/jdk1.8.0_241/ jdk More info: Generating(3)配置环境变量添加jdk配置： 12345vi /etc/profile#JDKexport JAVA_HOME=/export/server/jdk1.8.0_241export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar (4)重新加载环境变量并验证： 12source /etc/profilejava -version [root@node1 ~]# java -versionjava version “1.8.0_241”Java(TM) SE Runtime Environment (build 1.8.0_241-b07)Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)(5)分发JDK相关文件到node2，node31.jdk相关文件： 12scp -r /export/servers/jdk1.8.0_171/ root@node2:/export/servers/ scp -r /export/servers/jdk1.8.0_171/ root@node3:/export/servers/ 2.系统环境变量： 12scp -r /etc/profile root@node2:/etc/profile scp -r /etc/profile root@node3:/etc/profile 3.分别在node2、3重新使环境变量生效 1source /etc/profile 三、Hadoop集群的安装和配置(1)上传 Hadoop 安装包并解压 1tar zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz (2)修改配置文件 1cd /export/server/hadoop-3.3.0/etc/hadoop 1.hadoop-env.sh 12345678export JAVA_HOME=/export/server/jdk1.8.0_241#Hadoop各个组件启动运行身份export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root 2.core-site.xml 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt;&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 单位：分 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.hdfs-site.xml 1234567&lt;configuration&gt;&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node2:9868&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 4.mapred-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;configuration&gt;&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 5.yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt;&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; (3)将hadoop添加到环境变量(三台) 1234vi /etc/profile#HADOOP_HOMEexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 重新加载环境变量： 1source /etc/profile (4)向node2、3 分发hadoop目录 12scp -r /export/server/hadoop-3.3.0/ root@node2:/export/server/hadoop-3.3.0/scp -r /export/server/hadoop-3.3.0/ root@node3:/export/server/hadoop-3.3.0/ (5)在node1格式化hdfs 1hdfs namenode -format (6)启动hadoop集群 12start-dfs.shstart-yarn.sh （7）jps查看进程：[root@node2 ~]# jps5697 Jps4867 DataNode2183 QuorumPeerMain5065 NodeManager4973 SecondaryNameNodeWeb UI页面验证：yarn: http:&#x2F;&#x2F;主节点ip:9868&#x2F;hdfs: http:&#x2F;&#x2F;主节点ip:8088&#x2F;More info: Deployment 四、Zookeeper集群安装（1）上传、解压、安装zookeeper 1tar -zxvf /export/servers/zookeeper-3.4.6.tar.gr -C /export/servers/ 建立软连接 1ln -s zookeeper/zookeeper-3.4.6 （2）修改配置文件1.zoo.cfg 1234cd /export/server/zookeeper/conf/cp zoo_sample.cfg zoo.cfg #修改文件名mkdir -p /export/server/zookeeper/zkdatas/vim zoo.cfg 修改如下： 12345678910#Zookeeper的数据存放目录dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照autopurge.snapRetainCount=3# 日志多少小时清理一次autopurge.purgeInterval=1# 集群中服务器地址server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 2.添加myid的配置在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 12mkdir -p /export/server/zookeeper/zkdatas/echo 1 &gt; /export/server/zookeeper/zkdatas/myid 分发myid 123cd /export/server/scp -r /export/server/zookeeper-3.4.6/ node2:$PWD scp -r /export/server/zookeeper-3.4.6/ node3:$PWD 3.在node2,node3修改myid的值，并建立软连接node2： 123cd /export/server/ln -s zookeeper-3.4.6/ zookeeper echo 2 &gt; /export/server/zookeeper/zkdatas/myid #node2 myid值为2 node3： 123cd /export/server/ln -s zookeeper-3.4.6/ zookeeper echo 3 &gt; /export/server/zookeeper/zkdatas/myid #node3 myid值为3 (3)添加环境变量(三台)： 1234vi /etc/profile#ZOOKEEPER_HOMEexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 重新加载环境变量： 1source /etc/profile （4）启动zookeeper（三台） 12/export/server/zookeeper/bin/zkServer.sh start #启动 /export/server/zookeeper/bin/zkServer.sh status #查看启动状态","categories":[],"tags":[]}],"categories":[],"tags":[]}